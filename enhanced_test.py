#!/usr/bin/env python3
"""
Enhanced RAG Experiment with Quality Evaluation

This module provides comprehensive quality assessment for RAG experiments,
comparing four different approaches: LLM-only, LLM+history, RAG, and RAG+history.
"""

import json
import os
from datetime import datetime
from experiments.experiment_runner import ExperimentRunner
from src.evaluator import ResponseEvaluator
from src.llm_handler import LLMHandler

def load_scenarios():
    """Load test scenarios from JSON file"""
    with open('data/scenarios.json', 'r', encoding='utf-8') as f:
        data = json.load(f)
        return data.get('scenarios', [])

def evaluate_response_quality(response, domain, question):
    """Evaluate response quality using domain-specific criteria"""
    quality_metrics = {
        "relevance_score": 0,
        "completeness_score": 0, 
        "specificity_score": 0,
        "practical_value": 0
    }
    
    response_lower = response.lower()
    
    # Domain-specific quality criteria
    domain_criteria = {
        "ì˜ë£Œìƒë‹´": {
            "relevant_terms": ["ì¦ìƒ", "ì¹˜ë£Œ", "ìš´ë™", "ìƒí™œìŠµê´€", "ê·¼ë ¥", "ì²´ì¤‘", "ìŠ¤íŠ¸ë ˆì¹­", "ê´€ì ˆ"],
            "specific_terms": ["ëŒ€í‡´ì‚¬ë‘ê·¼", "ì—°ê³¨", "ë¬´ë¦", "ë¬¼ë¦¬ì¹˜ë£Œ", "ëƒ‰ì°œì§ˆ", "ì˜¨ì°œì§ˆ"],
            "practical_terms": ["ë°©ë²•", "ìš´ë™ë²•", "ì£¼ì˜ì‚¬í•­", "ë‹¨ê³„ë³„", "ì¼ìƒ"]
        },
        "ê¸°ìˆ ì§€ì›": {
            "relevant_terms": ["ì˜¤ë¥˜", "í•´ê²°", "ì„¤ì¹˜", "ì„¤ì •", "ì½”ë“œ", "ë²„ì „", "ì‹œìŠ¤í…œ"],
            "specific_terms": ["dockerfile", "_ctypes", "python", "pip", "flask", "gunicorn"],
            "practical_terms": ["í•´ê²°ë²•", "ë‹¨ê³„", "ëª…ë ¹ì–´", "ë°©ë²•", "ì„¤ì •"]
        },
        "íˆ¬ììƒë‹´": {
            "relevant_terms": ["íˆ¬ì", "í¬íŠ¸í´ë¦¬ì˜¤", "ë¶„ì‚°", "ìˆ˜ìµ", "ìœ„í—˜", "ìì‚°", "í€ë“œ"],
            "specific_terms": ["etf", "êµ­ë‚´ì£¼ì‹", "í•´ì™¸ì£¼ì‹", "ì±„ê¶Œ", "ë¦¬ë°¸ëŸ°ì‹±", "ëª©í‘œ"], 
            "practical_terms": ["ë¹„ìœ¨", "ë°©ë²•", "ì „ëµ", "ê³„íš", "ë‹¨ê³„"]
        }
    }
    
    criteria = domain_criteria.get(domain, domain_criteria["ì˜ë£Œìƒë‹´"])
    
    # ê´€ë ¨ì„± ì ìˆ˜ (0-10)
    relevant_count = sum(1 for term in criteria["relevant_terms"] if term in response_lower)
    quality_metrics["relevance_score"] = min(10, relevant_count * 1.5)
    
    # êµ¬ì²´ì„± ì ìˆ˜ (0-10)  
    specific_count = sum(1 for term in criteria["specific_terms"] if term in response_lower)
    quality_metrics["specificity_score"] = min(10, specific_count * 2)
    
    # ì‹¤ìš©ì„± ì ìˆ˜ (0-10)
    practical_count = sum(1 for term in criteria["practical_terms"] if term in response_lower)
    quality_metrics["practical_value"] = min(10, practical_count * 2)
    
    # ì™„ì„±ë„ ì ìˆ˜ (ì‘ë‹µ ê¸¸ì´ ë° êµ¬ì¡° ê¸°ë°˜)
    word_count = len(response.split())
    has_structure = "1." in response or "**" in response or "###" in response
    quality_metrics["completeness_score"] = min(10, (word_count / 50) + (3 if has_structure else 0))
    
    # ì „ì²´ í’ˆì§ˆ ì ìˆ˜
    total_score = sum(quality_metrics.values()) / 4
    quality_metrics["overall_quality"] = round(total_score, 2)
    
    return quality_metrics

def run_enhanced_experiment():
    """Run comprehensive experiment with quality evaluation"""
    print("=" * 60)
    print("RAG ì‹¤í—˜ í”Œë«í¼ - í’ˆì§ˆ ì¤‘ì‹¬ í‰ê°€")
    print("=" * 60)
    
    # API í‚¤ í™•ì¸
    openai_key = os.getenv('OPENAI_API_KEY')
    tavily_key = os.getenv('TAVILY_API_KEY')
    
    if not openai_key:
        print("âŒ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return
    
    print("âœ… OpenAI API í‚¤ í™•ì¸ë¨")
    
    if tavily_key:
        print("âœ… Tavily API í‚¤ ì„¤ì •ë¨ - RAG ê¸°ëŠ¥ í™œì„±í™”")
        search_enabled = True
    else:
        print("âš ï¸  Tavily API í‚¤ ì—†ìŒ - ê²€ìƒ‰ ì—†ëŠ” RAG í…ŒìŠ¤íŠ¸")
        search_enabled = False
    
    # ì‹œë‚˜ë¦¬ì˜¤ ë¡œë“œ
    scenarios = load_scenarios()
    print(f"âœ… {len(scenarios)}ê°œ ì‹œë‚˜ë¦¬ì˜¤ ë¡œë“œë¨")
    
    # ì‹¤í—˜ ëŸ¬ë„ˆ ì´ˆê¸°í™”
    runner = ExperimentRunner(
        openai_key=openai_key,
        tavily_key=tavily_key if search_enabled else None
    )
    print("âœ… ì‹¤í—˜ ëŸ¬ë„ˆ ì´ˆê¸°í™” ì™„ë£Œ")
    
    results = []
    
    # ì²« 3ê°œ ì‹œë‚˜ë¦¬ì˜¤ë¡œ í…ŒìŠ¤íŠ¸
    test_scenarios = scenarios[:3]
    
    for i, scenario in enumerate(test_scenarios, 1):
        print(f"\n{'='*50}")
        print(f"ì‹œë‚˜ë¦¬ì˜¤ {i}/{len(test_scenarios)}: {scenario['domain']}")
        print(f"{'='*50}")
        print(f"ì§ˆë¬¸: {scenario['current_question']}")
        print(f"íˆìŠ¤í† ë¦¬: {len(scenario.get('conversation_history', []))}í„´")
        
        # 4ê°€ì§€ ë°©ë²•ìœ¼ë¡œ ì‹¤í—˜ ì‹¤í–‰
        experiment_results = runner.run_all_methods(scenario)
        
        # í’ˆì§ˆ í‰ê°€ ì¶”ê°€
        quality_analysis = {}
        response_comparison = {}
        
        for method, result in experiment_results.items():
            if 'response' in result:
                # í’ˆì§ˆ í‰ê°€
                quality_metrics = evaluate_response_quality(
                    result['response'], 
                    scenario['domain'], 
                    scenario['current_question']
                )
                quality_analysis[method] = quality_metrics
                
                # ì‘ë‹µ ë¹„êµë¥¼ ìœ„í•œ ìš”ì•½
                response_preview = result['response'][:100] + "..." if len(result['response']) > 100 else result['response']
                
                response_comparison[method] = {
                    "length": len(result['response']),
                    "preview": response_preview,
                    "quality_score": quality_metrics["overall_quality"],
                    "tokens": result.get('usage', {}).get('total_tokens', 0),
                    "search_results": len(result.get('search_results', [])),
                    "has_search": len(result.get('search_results', [])) > 0
                }
                
                print(f"\nğŸ”„ {method} ì™„ë£Œ:")
                print(f"   í’ˆì§ˆì ìˆ˜: {quality_metrics['overall_quality']}/10")
                print(f"   ì‘ë‹µê¸¸ì´: {len(result['response'])}ì")
                print(f"   í† í°ì‚¬ìš©: {result.get('usage', {}).get('total_tokens', 0)}ê°œ")
                if result.get('search_results'):
                    print(f"   ê²€ìƒ‰ê²°ê³¼: {len(result['search_results'])}ê°œ")
        
        # ê²°ê³¼ ì €ì¥
        scenario_result = {
            "scenario_id": scenario["id"],
            "domain": scenario["domain"],
            "question": scenario["current_question"],
            "timestamp": datetime.now().isoformat(),
            "experiment_results": experiment_results,
            "quality_analysis": quality_analysis,
            "response_comparison": response_comparison
        }
        
        results.append(scenario_result)
        
        # ì‹œë‚˜ë¦¬ì˜¤ë³„ í’ˆì§ˆ ë¹„êµ ì¶œë ¥
        print(f"\nğŸ“Š í’ˆì§ˆ ë¹„êµ ê²°ê³¼:")
        methods = ["D1", "D2", "D3", "D4"]
        method_names = ["LLMë§Œ", "LLM+íˆìŠ¤í† ë¦¬", "RAG", "RAG+íˆìŠ¤í† ë¦¬"]
        
        for method, name in zip(methods, method_names):
            if method in quality_analysis:
                quality = quality_analysis[method]
                comparison = response_comparison[method]
                print(f"  {name}: {quality['overall_quality']}/10ì ")
                print(f"    â”” ê´€ë ¨ì„±:{quality['relevance_score']:.1f} êµ¬ì²´ì„±:{quality['specificity_score']:.1f} ì‹¤ìš©ì„±:{quality['practical_value']:.1f}")
                if comparison['has_search']:
                    print(f"    â”” ê²€ìƒ‰ í™œìš©: {comparison['search_results']}ê°œ ê²°ê³¼")
    
    # ì „ì²´ ê²°ê³¼ ë¶„ì„
    print(f"\n{'='*60}")
    print("ğŸ“ˆ ì „ì²´ ì‹¤í—˜ ê²°ê³¼ ë¶„ì„")
    print(f"{'='*60}")
    
    # ë°©ë²•ë³„ í‰ê·  í’ˆì§ˆ ì ìˆ˜
    method_averages = {}
    for method in ["D1", "D2", "D3", "D4"]:
        scores = []
        for result in results:
            if method in result["quality_analysis"]:
                scores.append(result["quality_analysis"][method]["overall_quality"])
        if scores:
            method_averages[method] = sum(scores) / len(scores)
    
    method_names = {
        "D1": "LLMë§Œ (íˆìŠ¤í† ë¦¬ ì—†ìŒ)",
        "D2": "LLM + íˆìŠ¤í† ë¦¬", 
        "D3": "RAG (í˜„ì¬ ì§ˆë¬¸ë§Œ)",
        "D4": "RAG + íˆìŠ¤í† ë¦¬"
    }
    
    print("ğŸ† ë°©ë²•ë³„ í‰ê·  í’ˆì§ˆ ì ìˆ˜:")
    for method, avg_score in sorted(method_averages.items(), key=lambda x: x[1], reverse=True):
        print(f"  {method_names[method]}: {avg_score:.2f}/10ì ")
    
    # ê°€ì„¤ ê²€ì¦
    print(f"\nğŸ”¬ ê°€ì„¤ ê²€ì¦ ê²°ê³¼:")
    if "D3" in method_averages and "D1" in method_averages:
        h1_result = method_averages["D3"] > method_averages["D1"]
        print(f"  H1 (RAG > LLM-only): {'âœ… ê²€ì¦ë¨' if h1_result else 'âŒ ê¸°ê°ë¨'}")
        print(f"    RAG: {method_averages.get('D3', 0):.2f} vs LLM-only: {method_averages.get('D1', 0):.2f}")
    
    if "D4" in method_averages and "D3" in method_averages:
        h2_result = method_averages["D4"] > method_averages["D3"]
        print(f"  H2 (íˆìŠ¤í† ë¦¬ ë°˜ì˜ > ë‹¨ìˆœ): {'âœ… ê²€ì¦ë¨' if h2_result else 'âŒ ê¸°ê°ë¨'}")
        print(f"    RAG+íˆìŠ¤í† ë¦¬: {method_averages.get('D4', 0):.2f} vs RAG: {method_averages.get('D3', 0):.2f}")
    
    # ê²°ê³¼ ì €ì¥
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = f"data/enhanced_test_results_{timestamp}.json"
    
    final_results = {
        "experiment_type": "enhanced_quality_evaluation",
        "timestamp": datetime.now().isoformat(),
        "total_scenarios": len(results),
        "search_enabled": search_enabled,
        "method_averages": method_averages,
        "hypothesis_results": {
            "H1_RAG_better_than_LLM": method_averages.get("D3", 0) > method_averages.get("D1", 0),
            "H2_history_better_than_simple": method_averages.get("D4", 0) > method_averages.get("D3", 0)
        },
        "detailed_results": results
    }
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(final_results, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ“ ê²°ê³¼ ì €ì¥ë¨: {output_file}")
    
    print(f"\n{'='*60}")
    print("âœ… í–¥ìƒëœ í’ˆì§ˆ í‰ê°€ ì‹¤í—˜ ì™„ë£Œ!")
    print("ì´ì œ êµìˆ˜ë‹˜ê»˜ ì˜ë¯¸ ìˆëŠ” ì—°êµ¬ ê²°ê³¼ë¥¼ ë³´ê³ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    print(f"{'='*60}")

if __name__ == "__main__":
    run_enhanced_experiment()